{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, applications\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from old_model import *\n",
    "from new_model import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plot functions and learning rate schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot_max(x,y, ax=None):\n",
    "    \"\"\" Annotate Maximum values of currently open plot\n",
    "    \"\"\"\n",
    "    xmax = x[np.argmax(y)]\n",
    "    ymax = y.max()\n",
    "    text= \"x={:.3f}, y={:.3f}\".format(xmax, ymax)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=60\")\n",
    "    kw = dict(xycoords='data',textcoords=\"data\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"left\", va=\"top\")\n",
    "    ax.annotate(text, xy=(xmax, ymax), xytext=(xmax-.025, ymax-.025), **kw)\n",
    "    \n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 0.9*epoch:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 0.8*epoch:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 0.6*epoch:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 0.4*epoch:\n",
    "        lr *= 1e-1\n",
    "    else: \n",
    "        lr = 1e-3\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifarFPNRes50e100(image_shape, class_number):\n",
    "    \"\"\"\n",
    "        Single output FPN fed to a Dense layer to accommodate recognition with Cifar-10.\n",
    "        Uses ResNet50 as backbone\n",
    "    \"\"\"\n",
    "    resnet50Backbone = get_backbone_ResNet50(input_shape=image_shape)\n",
    "    model = customFeaturePyramid2(resnet50Backbone, class_number)\n",
    "    return model\n",
    "\n",
    "def cifarFPNRes50V2e100(image_shape, class_number):\n",
    "    resnet50V2Backbone = get_backbone_ResNet50V2(input_shape=image_shape)\n",
    "    model = customFeaturePyramid2(resnet50V2Backbone, class_number)\n",
    "    return model\n",
    "\n",
    "def cifarFPNRes101e200(image_shape, class_number):\n",
    "    resnet101Backbone = get_backbone_ResNet101(input_shape=image_shape)\n",
    "    model = customFeaturePyramid2(resnet101Backbone, class_number)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "#               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "batch_size = 16  # orig paper trained all networks with batch_size=128\n",
    "epochs = 200\n",
    "########## Specify models to train and test with cifar-10 here\n",
    "model_list = [cifarFPNRes50e100, cifarFPNRes101e200]\n",
    "num_classes = 10\n",
    "\n",
    "(train_img, train_lab), (test_img, test_lab) = datasets.cifar10.load_data() #load dataset\n",
    "train_img, test_img = train_img/255.0, test_img/255.0 #normalize dataset\n",
    "# convert label to binary encoding\n",
    "train_lab = keras.utils.to_categorical(train_lab, num_classes)\n",
    "test_lab = keras.utils.to_categorical(test_lab, num_classes)\n",
    "val_img  = train_img[40000:]\n",
    "val_lab = train_lab[40000:]\n",
    "train_img = train_img[0:40000]\n",
    "train_lab = train_lab[0:40000]\n",
    "# print(train_img.shape,test_img.shape)\n",
    "img_shape = train_img.shape[1:]\n",
    "img_rows = train_img[0].shape[0]\n",
    "img_cols = train_img[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = []\n",
    "test_acc = []\n",
    "time_taken = []\n",
    "# opt = 'adam'\n",
    "# loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = Adam(learning_rate = lr_schedule(epochs))\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "for f in model_list:\n",
    "    model = f(img_shape, num_classes)\n",
    "#     model.build(img_shape)\n",
    "#     model.summary()\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=loss,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_img,\n",
    "                            train_lab,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(test_img, test_lab),\n",
    "                            shuffle=True)\n",
    "    ########## save model\n",
    "    #https://stackoverflow.com/questions/51806852/cant-save-custom-subclassed-model\n",
    "    model.save_weights('{}_epoch{}_batch{}_weights'.format(f.__name__, epochs, batch_size), save_format='tf')\n",
    "    \n",
    "    ########## plot stuff\n",
    "    plt.style.use('seaborn')\n",
    "    x = [i for i in range(1, epochs+1)]\n",
    "    fig = plt.figure()\n",
    "    ########## plot model training accuracy\n",
    "    plt.subplot(211)\n",
    "    acc = np.array(history.history['accuracy'])\n",
    "    valAcc = np.array(history.history['val_accuracy'])\n",
    "    plt.plot(acc)\n",
    "    plt.plot(valAcc)\n",
    "    annot_max(x, acc)\n",
    "    annot_max(x, valAcc)\n",
    "    plt.legend(['train', 'test'], loc='lower right')\n",
    "    plt.title('{}_trainAcc'.format(f.__name__))\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    ########## plot model training loss\n",
    "    plt.subplot(212)\n",
    "    loss = np.array(history.history['loss'])\n",
    "    valLoss = np.array(history.history['val_loss'])\n",
    "    plt.plot(loss)\n",
    "    plt.plot(valLoss)\n",
    "    annot_max(x, loss)\n",
    "    annot_max(x, valLoss)\n",
    "    plt.legend(['train', 'test'], loc='lower right')\n",
    "    plt.title('{}_trainLoss'.format(f.__name__))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "    plt.savefig('{}_trainAccLoss'.format(f.__name__), format='png')\n",
    "    plt.close(plt.gcf())\n",
    "\n",
    "    time1 = time.time()\n",
    "    a, b = model.evaluate(test_img, test_lab, verbose=2)\n",
    "    time_taken.append(time.time() - time1)\n",
    "    test_loss.append(a)\n",
    "    test_acc.append(b)\n",
    "\n",
    "print(test_loss)\n",
    "print(test_acc)\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
